{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNH9IXsl57JmzJWYHUQEZOk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Frodo-Swaggins/COMP702PROJECT/blob/main/NovelConformerForSpeakerIdentificationWithDWSC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GzvdUsA5IlHw",
        "outputId": "a36f8b12-c46e-4a7c-ef95-66a1dd6ba1ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Step 2: Import tarfile and extract the file\n",
        "import tarfile\n",
        "\n",
        "# Specify the path to the .tar.gz file in your Google Drive\n",
        "file_path = '/content/drive/MyDrive/train-other-500.tar.gz'  # Change this to the actual path of your .tar.gz file in Google Drive\n",
        "\n",
        "# Specify the destination folder where you want to extract the contents\n",
        "destination_path = '/content/train-other-500-uncompressed/'  # You can change this to your desired destination folder\n",
        "\n",
        "# Open the .tar.gz file and extract all contents\n",
        "with tarfile.open(file_path, 'r:gz') as tar:\n",
        "    tar.extractall(path=destination_path)\n",
        "\n",
        "# Verify by listing the contents of the destination folder\n",
        "import os\n",
        "print(os.listdir(destination_path))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "\n",
        "def add_noise(audio, noise_factor=0.005):\n",
        "    \"\"\"Adds white noise to the audio signal.\"\"\"\n",
        "    noise = np.random.randn(len(audio))  # Generate random noise\n",
        "    augmented_audio = audio + noise_factor * noise  # Add noise to the audio signal\n",
        "    return augmented_audio\n",
        "\n",
        "\n",
        "\n",
        "def create_noisy_dataset(original_folder, noisy_folder, noise_factor):\n",
        "    \"\"\"\n",
        "    Creates a noisy dataset by adding noise to each audio file in the original dataset,\n",
        "    and saves it directly to Google Drive.\n",
        "\n",
        "    Args:\n",
        "        original_folder (str): Path to the folder with the original clean dataset.\n",
        "        noisy_folder (str): Path to the folder in Google Drive where the noisy dataset will be saved.\n",
        "        noise_factor (float): Factor to control the amount of noise added.\n",
        "    \"\"\"\n",
        "    # Ensure the noisy folder exists in Google Drive\n",
        "    if not os.path.exists(noisy_folder):\n",
        "        os.makedirs(noisy_folder)\n",
        "\n",
        "    # Loop through all .flac files in the original folder\n",
        "    for root, dirs, files in os.walk(original_folder):\n",
        "        for file in files:\n",
        "            if file.endswith('.flac'):  # Process only .flac files\n",
        "                file_path = os.path.join(root, file)\n",
        "\n",
        "                # Load the original audio\n",
        "                audio, sr = librosa.load(file_path, sr=None)\n",
        "\n",
        "                # Add noise to the audio\n",
        "                noisy_audio = add_noise(audio, noise_factor=noise_factor)\n",
        "\n",
        "                # Create the corresponding path for the noisy audio\n",
        "                relative_path = os.path.relpath(root, original_folder)\n",
        "                noisy_subfolder = os.path.join(noisy_folder, relative_path)\n",
        "\n",
        "                # Ensure the subfolder exists in Google Drive\n",
        "                if not os.path.exists(noisy_subfolder):\n",
        "                    os.makedirs(noisy_subfolder)\n",
        "\n",
        "                # Save the noisy audio directly to Google Drive\n",
        "                noisy_file_path = os.path.join(noisy_subfolder, file)\n",
        "                sf.write(noisy_file_path, noisy_audio, sr)\n",
        "\n",
        "                print(f\"Created noisy file: {noisy_file_path}\")\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to initialize paths and create the noisy dataset directly in Google Drive.\n",
        "    \"\"\"\n",
        "    # Path to the original dataset on Colab\n",
        "    original_folder = '/content/train-other-500-uncompressed/LibriSpeech'\n",
        "\n",
        "    # Path to the noisy dataset folder in Google Drive\n",
        "    noisy_folder = '/content/drive/MyDrive/noisy_dataset'  # This will save directly to Google Drive\n",
        "\n",
        "    # Set noise factor for augmentation\n",
        "    noise_factor = 0.05\n",
        "\n",
        "    # Create the noisy dataset directly in Google Drive\n",
        "    create_noisy_dataset(original_folder, noisy_folder, noise_factor)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "fJ6rc6baTPrH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchaudio"
      ],
      "metadata": {
        "id": "6KIf7st7TSjv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "print(\"Current device:\", torch.cuda.current_device())\n"
      ],
      "metadata": {
        "id": "96Oh32RRTUO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import librosa\n",
        "import numpy as np\n",
        "\n",
        "def remove_silence(audio, sr, top_db=30):\n",
        "    \"\"\"\n",
        "    Remove silence from the audio using a decibel threshold.\n",
        "    \"\"\"\n",
        "    trimmed_audio, _ = librosa.effects.trim(audio, top_db=top_db)\n",
        "    print(f\"Audio length after silence removal: {len(trimmed_audio)} samples\")\n",
        "    return trimmed_audio\n",
        "\n",
        "def extract_mfcc(audio, sr, segment_length_sec=4.0, overlap_fraction=0.5, n_mfcc=13):\n",
        "    \"\"\"\n",
        "    Extracts MFCCs from the given audio signal in fixed-length segments with overlap.\n",
        "    \"\"\"\n",
        "    segment_length_samples = int(segment_length_sec * sr)\n",
        "    overlap_samples = int(overlap_fraction * segment_length_samples)\n",
        "    step_size = segment_length_samples - overlap_samples\n",
        "\n",
        "    print(\"Extracting MFCCs...\")\n",
        "\n",
        "    mfccs = []\n",
        "    for start_idx in range(0, len(audio) - segment_length_samples + 1, step_size):\n",
        "        segment = audio[start_idx:start_idx + segment_length_samples]\n",
        "        mfcc = librosa.feature.mfcc(y=segment, sr=sr, n_mfcc=n_mfcc)\n",
        "        mfccs.append(mfcc)\n",
        "\n",
        "    print(f\"Extracted {len(mfccs)} MFCC segments\")\n",
        "    return mfccs\n",
        "\n",
        "def process_speaker_files(dataset_path, speaker_subfolder, max_mfcc_per_speaker=500, segment_length_sec=4.0, overlap_fraction=0.5, top_db=30):\n",
        "    \"\"\"\n",
        "    Process all .flac files for a given speaker (including subfolders), remove silence, and extract MFCCs.\n",
        "    \"\"\"\n",
        "    speaker_path = os.path.join(dataset_path, speaker_subfolder)\n",
        "    mfcc_segments = []\n",
        "\n",
        "    print(f\"Processing speaker folder: {speaker_path}\")\n",
        "\n",
        "    for root, dirs, files in os.walk(speaker_path):\n",
        "        print(f\"Searching in: {root}\")\n",
        "        for file in files:\n",
        "            if file.endswith('.flac'):\n",
        "                file_path = os.path.join(root, file)\n",
        "                print(f\"Processing file: {file_path}\")\n",
        "\n",
        "                audio, sr = librosa.load(file_path, sr=None)\n",
        "                print(f\"Audio loaded, length: {len(audio)} samples\")\n",
        "\n",
        "                if len(audio) == 0:\n",
        "                    print(f\"Warning: Empty audio file {file_path}\")\n",
        "                    continue\n",
        "\n",
        "                trimmed_audio = remove_silence(audio, sr, top_db=top_db)\n",
        "\n",
        "                if len(trimmed_audio) == 0:\n",
        "                    print(f\"Warning: Silence removal resulted in empty audio for {file_path}\")\n",
        "                    continue\n",
        "\n",
        "                mfccs = extract_mfcc(trimmed_audio, sr, segment_length_sec, overlap_fraction)\n",
        "                mfcc_segments.extend(mfccs)\n",
        "\n",
        "                if len(mfcc_segments) >= max_mfcc_per_speaker:\n",
        "                    mfcc_segments = mfcc_segments[:max_mfcc_per_speaker]\n",
        "                    break\n",
        "\n",
        "        if len(mfcc_segments) >= max_mfcc_per_speaker:\n",
        "            break\n",
        "\n",
        "    print(f\"Total MFCC segments for speaker {speaker_subfolder}: {len(mfcc_segments)}\")\n",
        "    return mfcc_segments\n",
        "\n",
        "def save_mfccs_to_drive(mfccs, output_folder, speaker_subfolder, dataset_type):\n",
        "    \"\"\"\n",
        "    Save each speaker's MFCCs as a .npy file with noisy or clean suffix in the speaker's folder.\n",
        "    \"\"\"\n",
        "    speaker_folder = os.path.join(output_folder, speaker_subfolder)\n",
        "    if not os.path.exists(speaker_folder):\n",
        "        os.makedirs(speaker_folder)\n",
        "        print(f\"Created folder: {speaker_folder}\")\n",
        "\n",
        "    output_file = os.path.join(speaker_folder, f\"{speaker_subfolder}_{dataset_type}_mfccs.npy\")\n",
        "    np.save(output_file, np.array(mfccs))\n",
        "    print(f\"Saved MFCCs for speaker {speaker_subfolder} ({dataset_type}) to {output_file}\")\n",
        "\n",
        "def process_datasets(noisy_dataset, clean_dataset, output_folder, flag, max_mfcc_per_speaker=500, segment_length_sec=4.0, overlap_fraction=0.5, top_db=30):\n",
        "    print(\"Starting dataset processing...\")\n",
        "    print(f\"Noisy dataset path: {noisy_dataset}\")\n",
        "    print(f\"Clean dataset path: {clean_dataset}\")\n",
        "\n",
        "    for speaker_subfolder in os.listdir(noisy_dataset):\n",
        "        noisy_speaker_path = os.path.join(noisy_dataset, speaker_subfolder)\n",
        "        clean_speaker_path = os.path.join(clean_dataset, speaker_subfolder)\n",
        "\n",
        "        if os.path.isdir(noisy_speaker_path) and os.path.isdir(clean_speaker_path):\n",
        "            print(f\"Processing speaker: {speaker_subfolder}\")\n",
        "\n",
        "            # Check if MFCC files already exist for this speaker\n",
        "            noisy_output_file = os.path.join(output_folder, speaker_subfolder, f\"{speaker_subfolder}_noisy_mfccs.npy\")\n",
        "            clean_output_file = os.path.join(output_folder, speaker_subfolder, f\"{speaker_subfolder}_clean_mfccs.npy\")\n",
        "\n",
        "            if os.path.exists(noisy_output_file) and os.path.exists(clean_output_file):\n",
        "                print(f\"Skipping speaker {speaker_subfolder} as MFCC files already exist.\")\n",
        "                continue\n",
        "\n",
        "            # Process the noisy dataset for this speaker with \"_noisy\" suffix\n",
        "            noisy_mfccs = process_speaker_files(noisy_dataset, speaker_subfolder, max_mfcc_per_speaker, segment_length_sec, overlap_fraction, top_db)\n",
        "            print(f\"Extracted {len(noisy_mfccs)} MFCCs from noisy dataset for speaker {speaker_subfolder}\")\n",
        "\n",
        "            # Save MFCCs for noisy dataset with \"noisy\" suffix\n",
        "            save_mfccs_to_drive(noisy_mfccs, output_folder, speaker_subfolder, dataset_type='noisy')\n",
        "\n",
        "            # Process the clean dataset for this speaker with \"_clean\" suffix\n",
        "            clean_mfccs = process_speaker_files(clean_dataset, speaker_subfolder, max_mfcc_per_speaker, segment_length_sec, overlap_fraction, top_db)\n",
        "            print(f\"Extracted {len(clean_mfccs)} MFCCs from clean dataset for speaker {speaker_subfolder}\")\n",
        "\n",
        "            # Save MFCCs for clean dataset with \"clean\" suffix\n",
        "            save_mfccs_to_drive(clean_mfccs, output_folder, speaker_subfolder, dataset_type='clean')\n",
        "\n",
        "\n",
        "noisy_dataset_path = '/content/drive/MyDrive/noisy_dataset/train-other-500'\n",
        "clean_dataset_path = '/content/train-other-500-uncompressed/LibriSpeech/train-other-500'\n",
        "output_folder = '/content/drive/MyDrive/mfcc_datasets_new'\n",
        "\n",
        "process_datasets(noisy_dataset_path, clean_dataset_path, output_folder)\n"
      ],
      "metadata": {
        "id": "_HFh_78jTo6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Check if a GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Set paths\n",
        "dataset_path = '/content/drive/MyDrive/mfcc_datasets_new'  # Replace with the path to your dataset\n",
        "target_length = 500  # Target number of segments per MFCC\n",
        "\n",
        "# Loop through each speaker folder\n",
        "for speaker_folder in os.listdir(dataset_path):\n",
        "    speaker_path = os.path.join(dataset_path, speaker_folder)\n",
        "\n",
        "    # Check if the path is a directory\n",
        "    if os.path.isdir(speaker_path):\n",
        "        # Loop through all .npy files in the speaker folder\n",
        "        for mfcc_file in os.listdir(speaker_path):\n",
        "            # Process only files without '_padded' or '_mask' in the filename\n",
        "            if mfcc_file.endswith('.npy') and not mfcc_file.endswith('_padded.npy') and not mfcc_file.endswith('_mask.npy'):\n",
        "                mfcc_file_path = os.path.join(speaker_path, mfcc_file)\n",
        "\n",
        "                # Define paths for padded and mask files\n",
        "                padded_mfcc_path = os.path.join(speaker_path, f\"{os.path.splitext(mfcc_file)[0]}_padded.npy\")\n",
        "                mask_path = os.path.join(speaker_path, f\"{os.path.splitext(mfcc_file)[0]}_mask.npy\")\n",
        "\n",
        "                # Load the MFCC file and move it to GPU\n",
        "                mfcc = torch.tensor(np.load(mfcc_file_path), device=device)  # Shape should be (segments, n_mfcc, features)\n",
        "\n",
        "                # Get current number of segments (1st dimension) and features (3rd dimension)\n",
        "                current_length = mfcc.shape[0]\n",
        "                n_features = mfcc.shape[2]  # Number of MFCC features per segment\n",
        "\n",
        "                # Create a mask with shape (target_length, n_mfcc, n_features)\n",
        "                mask = torch.ones((target_length, mfcc.shape[1], n_features), dtype=torch.bool, device=device)\n",
        "\n",
        "                if current_length < target_length:\n",
        "                    # Pad the MFCC to the target length along the first dimension (segments)\n",
        "                    padded_mfcc = F.pad(mfcc, (0, 0, 0, 0, 0, target_length - current_length), \"constant\", 0)\n",
        "                    # Update the mask to mark padded segments\n",
        "                    mask[current_length:, :, :] = False\n",
        "                else:\n",
        "                    # Truncate if the MFCC length is longer than the target length\n",
        "                    padded_mfcc = mfcc[:target_length, :, :]\n",
        "\n",
        "                # Move data back to CPU for saving\n",
        "                padded_mfcc = padded_mfcc.cpu().numpy()\n",
        "                mask = mask.cpu().numpy()\n",
        "\n",
        "                # Save the padded MFCC and mask, overwriting existing files if they exist\n",
        "                np.save(padded_mfcc_path, padded_mfcc)\n",
        "                np.save(mask_path, mask)\n",
        "\n",
        "                print(f\"Processed {mfcc_file}: Saved and overwritten padded MFCC and mask.\")\n"
      ],
      "metadata": {
        "id": "1KP87nJNTyRL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchaudio\n",
        "\n",
        "# Swish Activation Function\n",
        "class Swish(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return x * torch.sigmoid(x)\n",
        "\n",
        "# Updated ConvSubsampling layer to handle channel dimension properly\n",
        "class ConvSubsampling(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(ConvSubsampling, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=2, padding=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.out_channels = out_channels\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x should be of shape (batch, 1, time, features)\n",
        "        x = x.unsqueeze(1)  # Add channel dimension if missing, resulting in (batch, 1, time, features)\n",
        "        x = self.conv(x)  # Perform subsampling\n",
        "        b, c, t, f = x.size()\n",
        "        x = x.permute(0, 2, 1, 3).contiguous().view(b, t, c * f)  # Reshape to (batch, time, out_channels * features // 4)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "# Feed Forward Module\n",
        "class FeedForwardModule(nn.Module):\n",
        "    def __init__(self, d_model, expansion_factor=4, dropout=0.1):\n",
        "        super(FeedForwardModule, self).__init__()\n",
        "        self.layer_norm = nn.LayerNorm(d_model)\n",
        "        self.linear1 = nn.Linear(d_model, d_model * expansion_factor)\n",
        "        self.activation = Swish()\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.linear2 = nn.Linear(d_model * expansion_factor, d_model)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        x = self.layer_norm(x)\n",
        "        x = self.linear1(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.dropout1(x)\n",
        "        x = self.linear2(x)\n",
        "        x = self.dropout2(x)\n",
        "        return x + residual\n",
        "\n",
        "# Multi-Head Self-Attention Module with Relative Positional Embedding\n",
        "class MHSA(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, dropout=0.1):\n",
        "        super(MHSA, self).__init__()\n",
        "        self.layer_norm = nn.LayerNorm(d_model)\n",
        "        self.attention = nn.MultiheadAttention(d_model, n_heads, dropout=dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, attention_mask=None):\n",
        "        residual = x\n",
        "        x = self.layer_norm(x)\n",
        "        x, _ = self.attention(x, x, x)  # Self-attention\n",
        "        x = self.dropout(x)\n",
        "        return x + residual\n",
        "\n",
        "#Convolution Module\n",
        "class ConvolutionModule(nn.Module):\n",
        "    def __init__(self, d_model, kernel_size=31, dropout=0.1):\n",
        "        super(ConvolutionModule, self).__init__()\n",
        "        self.layer_norm = nn.LayerNorm(d_model)\n",
        "        self.pointwise_conv1 = nn.Conv1d(d_model, 2 * d_model, kernel_size=1)\n",
        "        self.glu = nn.GLU(dim=1)\n",
        "        self.depthwise_conv = nn.Conv1d(d_model, d_model, kernel_size=kernel_size, padding=(kernel_size - 1) // 2, groups=d_model)\n",
        "        self.batch_norm = nn.BatchNorm1d(d_model)\n",
        "        self.swish = Swish()\n",
        "        self.pointwise_conv2 = nn.Conv1d(d_model, d_model, kernel_size=1)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        x = self.layer_norm(x)\n",
        "        x = x.transpose(1, 2)  # (batch, d_model, time)\n",
        "        x = self.pointwise_conv1(x)\n",
        "        x = self.glu(x)\n",
        "        x = self.depthwise_conv(x)\n",
        "        x = self.batch_norm(x)\n",
        "        x = self.swish(x)\n",
        "        x = self.pointwise_conv2(x)\n",
        "        x = self.dropout(x)\n",
        "        x = x.transpose(1, 2)  # (batch, time, d_model)\n",
        "\n",
        "        # Adjust x to match the residual shape in the time dimension if necessary\n",
        "        if x.size(1) != residual.size(1):\n",
        "            min_time_dim = min(x.size(1), residual.size(1))\n",
        "            x = x[:, :min_time_dim, :]\n",
        "            residual = residual[:, :min_time_dim, :]\n",
        "            #print(f\"Adjusted shapes for residual connection: x.shape={x.shape}, residual.shape={residual.shape}\")\n",
        "\n",
        "        return x + residual\n",
        "\n",
        "\n",
        "# Conformer Block\n",
        "class ConformerBlock(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, kernel_size, dropout=0.1):\n",
        "        super(ConformerBlock, self).__init__()\n",
        "        self.ffn1 = FeedForwardModule(d_model, dropout=dropout)\n",
        "        self.mhsa = MHSA(d_model, n_heads, dropout=dropout)\n",
        "        self.conv = ConvolutionModule(d_model, kernel_size=kernel_size, dropout=dropout)\n",
        "        self.ffn2 = FeedForwardModule(d_model, dropout=dropout)\n",
        "        self.layer_norm = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.ffn1(x)\n",
        "        x = self.mhsa(x)\n",
        "        x = self.conv(x)\n",
        "        x = self.ffn2(x)\n",
        "        return self.layer_norm(x)\n",
        "\n",
        "class ConformerEncoder(nn.Module):\n",
        "    def __init__(self, input_dim, d_model, n_heads, num_blocks, kernel_size=31, dropout=0.1, num_classes=10):\n",
        "        super(ConformerEncoder, self).__init__()\n",
        "        self.subsampling = ConvSubsampling(input_dim, d_model)  # Initialize with given d_model\n",
        "        self.linear = nn.Linear(d_model, d_model)  # Initialize Linear layer; will set dynamically in forward\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Define Conformer Blocks\n",
        "        self.conformer_blocks = nn.ModuleList([\n",
        "            ConformerBlock(d_model, n_heads, kernel_size, dropout=dropout) for _ in range(num_blocks)\n",
        "        ])\n",
        "\n",
        "        # Output layer for classification\n",
        "        self.output_layer = nn.Linear(d_model, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Subsampling layer\n",
        "        x = self.subsampling(x)\n",
        "        # Capture the feature dimension after subsampling\n",
        "        batch_size, time, features = x.shape\n",
        "\n",
        "        # Dynamically set Linear layer input dimension if necessary\n",
        "        if self.linear.in_features != features:\n",
        "            self.linear = nn.Linear(features, d_model).to(x.device)  # Adjust Linear layer input size\n",
        "            print(f\"Dynamically adjusted Linear layer input size to: {features}\")\n",
        "\n",
        "        # Pass through Linear and Dropout layers\n",
        "        x = self.linear(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Pass through each Conformer block\n",
        "        for i, block in enumerate(self.conformer_blocks):\n",
        "            x = block(x)\n",
        "\n",
        "        # Global pooling and final output layer\n",
        "        x = self.output_layer(x.mean(dim=1))  # Pool across time dimension\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class DepthwiseSeparableConv(nn.Module):\n",
        "    def __init__(self, input_dim, d_model, kernel_size, bias=True):\n",
        "        super().__init__()\n",
        "        self.depthwise_conv = nn.Conv1d(in_channels=input_dim, out_channels=d_model, kernel_size=kernel_size, groups=input_dim, padding=kernel_size // 2, bias=False)\n",
        "        self.pointwise_conv = nn.Conv1d(in_channels=input_dim, out_channels=d_model, kernel_size=1, padding=0, bias=bias)\n",
        "    def forward(self, x):\n",
        "        return F.relu(self.pointwise_conv(self.depthwise_conv(x)))\n"
      ],
      "metadata": {
        "id": "QIjA6CvEUK2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import shutil\n",
        "\n",
        "# Define dataset path and split ratios\n",
        "data_path = '/content/drive/MyDrive/mfcc_datasets_new'  # Path to your MFCC dataset\n",
        "train_ratio, val_ratio = 0.8, 0.2\n",
        "\n",
        "# List all speaker directories and shuffle for randomness\n",
        "speakers = [d for d in os.listdir(data_path) if os.path.isdir(os.path.join(data_path, d))]\n",
        "random.shuffle(speakers)\n",
        "\n",
        "# Calculate split indices\n",
        "total_speakers = len(speakers)\n",
        "train_idx = int(train_ratio * total_speakers)\n",
        "\n",
        "# Split speakers\n",
        "train_speakers = speakers[:train_idx]\n",
        "val_speakers = speakers[train_idx:]\n",
        "\n",
        "# Function to copy speaker data to designated split folders\n",
        "def copy_speakers(speakers_list, subset_name):\n",
        "    subset_path = os.path.join(data_path, subset_name)\n",
        "    os.makedirs(subset_path, exist_ok=True)\n",
        "    for speaker in speakers_list:\n",
        "        shutil.copytree(os.path.join(data_path, speaker), os.path.join(subset_path, speaker), dirs_exist_ok=True)\n",
        "\n",
        "# Create train and val splits\n",
        "copy_speakers(train_speakers, 'train')\n",
        "copy_speakers(val_speakers, 'val')"
      ],
      "metadata": {
        "id": "JIiC3cVpUToa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "\n",
        "# Import necessary library for profiling FLOPs\n",
        "!pip install torchprofile\n",
        "import torchprofile"
      ],
      "metadata": {
        "id": "xS0CdoiVUWwt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Paths to save model checkpoints\n",
        "checkpoint_dir = '/content/drive/MyDrive/model_checkpoints/5'\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "# Define fixed dimensions\n",
        "target_height = 500  # Desired time dimension\n",
        "target_width = 126   # Desired feature dimension\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Function to calculate FLOPs and parameter count\n",
        "import torchprofile\n",
        "\n",
        "# Function to calculate FLOPs and parameter count\n",
        "def get_model_complexity(model, input_shape):\n",
        "    # Calculate MACs for model\n",
        "    macs = torchprofile.profile_macs(model, torch.randn(input_shape).to(device))\n",
        "    # Calculate total parameters for model\n",
        "    params = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"Model MACs: {macs / 1e9:.2f} GMACs, Parameters: {params / 1e6:.2f}M\")\n",
        "\n",
        "# Define checkpoint saving function\n",
        "def save_checkpoint(model, optimizer, epoch, train_loss, val_loss, filepath):\n",
        "    checkpoint = {\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'epoch': epoch,\n",
        "        'train_loss': train_loss,\n",
        "        'val_loss': val_loss\n",
        "    }\n",
        "    torch.save(checkpoint, filepath)\n",
        "    print(f\"Checkpoint saved: {filepath}\")\n",
        "\n",
        "# Loading from checkpoint if exists\n",
        "def load_checkpoint(checkpoint_path, model, optimizer):\n",
        "    import torch\n",
        "    import os\n",
        "\n",
        "    # Define the checkpoint path\n",
        "    checkpoint_path = os.path.join(checkpoint_path, 'latest_checkpoint.pth')\n",
        "    start_epoch = 0  # Default start epoch\n",
        "\n",
        "    # Attempt to load the checkpoint\n",
        "    if os.path.exists(checkpoint_path):\n",
        "        try:\n",
        "            print(f\"Attempting to load checkpoint from {checkpoint_path}\")\n",
        "            checkpoint = torch.load(checkpoint_path)\n",
        "\n",
        "            # Load model and optimizer states\n",
        "            model.load_state_dict(checkpoint['model_state_dict'])\n",
        "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "            start_epoch = checkpoint['epoch'] + 1  # Start from the next epoch\n",
        "            print(f\"Checkpoint loaded successfully, resuming from epoch {start_epoch}\")\n",
        "            return start_epoch\n",
        "        except RuntimeError as e:\n",
        "            # Handle the mismatch by creating a new model\n",
        "            print(f\"Checkpoint mismatch detected. Error: {e}\")\n",
        "            print(\"Creating a new model and starting from scratch.\")\n",
        "\n",
        "            # Reinitialize model and optimizer for a fresh start\n",
        "            model = ConformerEncoder(\n",
        "                input_dim=input_dim,\n",
        "                d_model=d_model,\n",
        "                n_heads=n_heads,\n",
        "                num_blocks=num_blocks,\n",
        "                kernel_size=kernel_size,\n",
        "                dropout=0.1,\n",
        "                num_classes=num_classes\n",
        "            ).to(device)\n",
        "            optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "            return 0  # Return 0 to start from scratch\n",
        "    else:\n",
        "        print(\"No checkpoint found. Starting training from scratch.\")\n",
        "        return start_epoch\n",
        "\n",
        "\n",
        "\n",
        "class SpeakerMFCCDataset(Dataset):\n",
        "    def __init__(self, root_dir):\n",
        "        self.root_dir = Path(root_dir)\n",
        "        self.samples = []\n",
        "\n",
        "        # Collect paths to all padded MFCC and mask files within each speaker folder\n",
        "        for speaker_folder in self.root_dir.iterdir():\n",
        "            if speaker_folder.is_dir():\n",
        "                for mfcc_file in speaker_folder.glob(\"*_padded.npy\"):\n",
        "                    mask_file = mfcc_file.with_name(mfcc_file.stem.replace(\"_padded\", \"_mask\") + \".npy\")\n",
        "                    if mask_file.exists():\n",
        "                        self.samples.append((mfcc_file, mask_file, speaker_folder.name))\n",
        "\n",
        "        # Create a label-to-index mapping\n",
        "        self.label_to_idx = {speaker: idx for idx, speaker in enumerate(set(s[2] for s in self.samples))}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        mfcc_path, mask_path, speaker = self.samples[idx]\n",
        "\n",
        "        # Load MFCC and mask\n",
        "        mfcc = torch.tensor(np.load(mfcc_path), dtype=torch.float32)\n",
        "        mask = torch.tensor(np.load(mask_path), dtype=torch.bool)\n",
        "\n",
        "        # Get label index from speaker name\n",
        "        label_idx = self.label_to_idx[speaker]\n",
        "\n",
        "        return mfcc, mask, label_idx\n",
        "\n",
        "# Collate function to enforce consistent shapes\n",
        "def collate_fn(batch):\n",
        "    mfccs, masks, labels = [], [], []\n",
        "    for mfcc, mask, label in batch:\n",
        "        # Ensure MFCC tensor has shape [1, time, features]\n",
        "        if mfcc.dim() == 2:\n",
        "            mfcc = mfcc.unsqueeze(0)\n",
        "        elif mfcc.dim() == 3 and mfcc.shape[0] != 1:\n",
        "            mfcc = mfcc.mean(dim=0, keepdim=True)\n",
        "\n",
        "        # Pad or crop to ensure [1, 500, 126] shape\n",
        "        mfcc = F.pad(mfcc, (0, target_width - mfcc.shape[-1], 0, target_height - mfcc.shape[-2]), mode='constant', value=0)\n",
        "        mfcc = mfcc[:, :target_height, :target_width]\n",
        "        mfccs.append(mfcc)\n",
        "\n",
        "        # Ensure mask tensor has shape [1, 500, 126]\n",
        "        if mask.dim() == 2:\n",
        "            mask = mask.unsqueeze(0)\n",
        "        elif mask.dim() == 3 and mask.shape[0] != 1:\n",
        "            mask = mask.float().mean(dim=0, keepdim=True)\n",
        "\n",
        "        mask = F.pad(mask, (0, target_width - mask.shape[-1], 0, target_height - mask.shape[-2]), mode='constant', value=0)\n",
        "        mask = mask[:, :target_height, :target_width]\n",
        "        masks.append(mask)\n",
        "\n",
        "        labels.append(label)\n",
        "\n",
        "    # Stack tensors and remove unnecessary dimensions\n",
        "    mfccs_batch = torch.stack(mfccs).squeeze(1)\n",
        "    masks_batch = torch.stack(masks).squeeze(1)\n",
        "    labels_batch = torch.tensor(labels)\n",
        "\n",
        "    return mfccs_batch, masks_batch, labels_batch\n",
        "\n",
        "\n",
        "# Initialize model, optimizer, and loss function\n",
        "input_dim = target_width\n",
        "d_model = 144\n",
        "n_heads = 4\n",
        "num_blocks = 8\n",
        "kernel_size = 32\n",
        "num_classes = 10  # Replace with actual number of classes\n",
        "model = ConformerEncoder(input_dim, d_model, n_heads, num_blocks, kernel_size, 0.1, num_classes).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "input_shape = (1, target_height, target_width)  # (batch_size, time, features)\n",
        "\n",
        "# Calculate and display model complexity before training\n",
        "print(\"Calculating initial model complexity:\")\n",
        "get_model_complexity(model, input_shape=input_shape)\n",
        "\n",
        "\n",
        "# Load checkpoint if available\n",
        "start_epoch = load_checkpoint(os.path.join(checkpoint_dir, 'latest_checkpoint.pth'), model, optimizer)\n",
        "\n",
        "# Training function with detailed logging and profiling\n",
        "def train_epoch(model, loader, optimizer, criterion, epoch):\n",
        "    model.train()\n",
        "    total_loss, correct = 0, 0\n",
        "    start_epoch_time = time.time()\n",
        "\n",
        "    for batch_idx, (data, labels) in enumerate(loader):\n",
        "        data, labels = data.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(data)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        correct += (outputs.argmax(dim=1) == labels).sum().item()\n",
        "\n",
        "        if batch_idx % 10 == 0:\n",
        "            print(f\"Epoch {epoch} - Batch {batch_idx}/{len(loader)}: Loss = {loss.item():.4f}\")\n",
        "\n",
        "    avg_loss = total_loss / len(loader)\n",
        "    accuracy = correct / len(loader.dataset)\n",
        "    epoch_duration = time.time() - start_epoch_time\n",
        "    print(f\"Epoch {epoch} completed in {epoch_duration / 60:.2f} mins\")\n",
        "\n",
        "    # Display computational metrics after each epoch\n",
        "    get_model_complexity(model, input_shape=(8, target_width))  # Adjust batch size and input shape\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "# Validation function\n",
        "def validate_epoch(model, loader, criterion, epoch):\n",
        "    model.eval()\n",
        "    total_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (data, labels) in enumerate(loader):\n",
        "            data, labels = data.to(device), labels.to(device)\n",
        "            outputs = model(data)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "            correct += (outputs.argmax(dim=1) == labels).sum().item()\n",
        "\n",
        "    avg_loss = total_loss / len(loader)\n",
        "    accuracy = correct / len(loader.dataset)\n",
        "    print(f\"Validation - Epoch {epoch}: Loss = {avg_loss:.4f}, Accuracy = {accuracy:.4f}\")\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "# Calculate and display model complexity before training\n",
        "print(\"Calculating initial model complexity:\")\n",
        "\n",
        "input_shape = (1, target_height, target_width)\n",
        "get_model_complexity(model, input_shape=input_shape)\n",
        "\n",
        "# Load checkpoint if available\n",
        "start_epoch = load_checkpoint(os.path.join(checkpoint_dir, 'latest_checkpoint.pth'), model, optimizer)\n",
        "\n",
        "\n",
        "# Example usage with train and val directories\n",
        "train_dataset = SpeakerMFCCDataset('/content/drive/MyDrive/mfcc_datasets_new/train')\n",
        "val_dataset = SpeakerMFCCDataset('/content/drive/MyDrive/mfcc_datasets_new/val')\n",
        "\n",
        "# Create DataLoaders with custom collate function\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=False,collate_fn=collate_fn)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False,collate_fn=collate_fn)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 20\n",
        "for epoch in range(start_epoch, num_epochs):\n",
        "    print(f\"Starting epoch {epoch+1}/{num_epochs}\")\n",
        "    train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, epoch)\n",
        "    val_loss, val_acc = validate_epoch(model, val_loader, criterion, epoch)\n",
        "\n",
        "    # Save checkpoint at the end of each epoch\n",
        "    save_checkpoint(model, optimizer, epoch, train_loss, val_loss, os.path.join(checkpoint_dir, 'latest_checkpoint.pth'))\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
        "    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n"
      ],
      "metadata": {
        "id": "QkztVbQkUjSN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}